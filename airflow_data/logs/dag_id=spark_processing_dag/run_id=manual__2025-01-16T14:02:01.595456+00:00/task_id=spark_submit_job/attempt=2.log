[2025-01-16T14:08:07.934+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_processing_dag.spark_submit_job manual__2025-01-16T14:02:01.595456+00:00 [queued]>
[2025-01-16T14:08:07.940+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_processing_dag.spark_submit_job manual__2025-01-16T14:02:01.595456+00:00 [queued]>
[2025-01-16T14:08:07.940+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-01-16T14:08:07.948+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_job> on 2025-01-16 14:02:01.595456+00:00
[2025-01-16T14:08:07.955+0000] {standard_task_runner.py:57} INFO - Started process 1240 to run task
[2025-01-16T14:08:07.958+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_processing_dag', 'spark_submit_job', 'manual__2025-01-16T14:02:01.595456+00:00', '--job-id', '68', '--raw', '--subdir', 'DAGS_FOLDER/spark-processing.py', '--cfg-path', '/tmp/tmptlryhtvc']
[2025-01-16T14:08:07.960+0000] {standard_task_runner.py:85} INFO - Job 68: Subtask spark_submit_job
[2025-01-16T14:08:07.998+0000] {task_command.py:416} INFO - Running <TaskInstance: spark_processing_dag.spark_submit_job manual__2025-01-16T14:02:01.595456+00:00 [running]> on host 183471e553b7
[2025-01-16T14:08:08.063+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='spark_processing_dag' AIRFLOW_CTX_TASK_ID='spark_submit_job' AIRFLOW_CTX_EXECUTION_DATE='2025-01-16T14:02:01.595456+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-01-16T14:02:01.595456+00:00'
[2025-01-16T14:08:08.071+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2025-01-16T14:08:08.072+0000] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.driver.host=spark-master --conf spark.driver.bindAddress=spark-master --name arrow-spark --verbose /opt/spark-apps/sample_spark_app.py /opt/spark-data/input /opt/spark-data/output
[2025-01-16T14:08:09.940+0000] {spark_submit.py:491} INFO - Using properties file: null
[2025-01-16T14:08:10.008+0000] {spark_submit.py:491} INFO - Parsed arguments:
[2025-01-16T14:08:10.008+0000] {spark_submit.py:491} INFO - master                  spark://spark-master:7077
[2025-01-16T14:08:10.008+0000] {spark_submit.py:491} INFO - deployMode              null
[2025-01-16T14:08:10.008+0000] {spark_submit.py:491} INFO - executorMemory          null
[2025-01-16T14:08:10.008+0000] {spark_submit.py:491} INFO - executorCores           null
[2025-01-16T14:08:10.008+0000] {spark_submit.py:491} INFO - totalExecutorCores      null
[2025-01-16T14:08:10.008+0000] {spark_submit.py:491} INFO - propertiesFile          null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - driverMemory            null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - driverCores             null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - driverExtraClassPath    null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - driverExtraLibraryPath  null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - driverExtraJavaOptions  null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - supervise               false
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - queue                   null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - numExecutors            null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - files                   null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - pyFiles                 null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - archives                null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - mainClass               null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - primaryResource         file:/opt/spark-apps/sample_spark_app.py
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - name                    arrow-spark
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - childArgs               [/opt/spark-data/input /opt/spark-data/output]
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - jars                    null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - packages                null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - packagesExclusions      null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - repositories            null
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - verbose                 true
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:10.009+0000] {spark_submit.py:491} INFO - Spark properties used, including those specified through
[2025-01-16T14:08:10.010+0000] {spark_submit.py:491} INFO - --conf and those from the properties file null:
[2025-01-16T14:08:10.010+0000] {spark_submit.py:491} INFO - (spark.driver.bindAddress,spark-master)
[2025-01-16T14:08:10.010+0000] {spark_submit.py:491} INFO - (spark.driver.host,spark-master)
[2025-01-16T14:08:10.010+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:10.010+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:10.354+0000] {spark_submit.py:491} INFO - Main class:
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - org.apache.spark.deploy.PythonRunner
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - Arguments:
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - file:/opt/spark-apps/sample_spark_app.py
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - null
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - /opt/spark-data/input
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - /opt/spark-data/output
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - Spark config:
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - (spark.app.name,arrow-spark)
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - (spark.app.submitTime,1737036490341)
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - (spark.driver.bindAddress,spark-master)
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - (spark.driver.host,spark-master)
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - (spark.master,spark://spark-master:7077)
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - (spark.submit.deployMode,client)
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - (spark.submit.pyFiles,)
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - Classpath elements:
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:10.355+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:10.356+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:11.182+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,181 - INFO - Starting Spark job
[2025-01-16T14:08:11.183+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,181 - INFO - Input path: /opt/spark-data/input
[2025-01-16T14:08:11.183+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,181 - INFO - Output path: /opt/spark-data/output
[2025-01-16T14:08:11.183+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,181 - INFO - Python version: 3.8.18 (default, Nov  1 2023, 11:13:53)
[2025-01-16T14:08:11.183+0000] {spark_submit.py:491} INFO - [GCC 10.2.1 20210110]
[2025-01-16T14:08:11.183+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,181 - INFO - Python executable: /usr/local/bin/python3
[2025-01-16T14:08:11.183+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,181 - INFO - Python path: ['/opt/spark-apps', '/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip', '/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip', '/home/***/.local/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.3.2.jar', '/opt/***/libraries', '/usr/local/lib/python38.zip', '/usr/local/lib/python3.8', '/usr/local/lib/python3.8/lib-dynload', '/home/***/.local/lib/python3.8/site-packages', '/usr/local/lib/python3.8/site-packages', '/opt/spark/python', '/opt/spark/python/lib/py4j-0.10.9.5-src.zip']
[2025-01-16T14:08:11.266+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SparkContext: Running Spark version 3.3.2
[2025-01-16T14:08:11.349+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-01-16T14:08:11.409+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ResourceUtils: ==============================================================
[2025-01-16T14:08:11.409+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-01-16T14:08:11.409+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ResourceUtils: ==============================================================
[2025-01-16T14:08:11.409+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SparkContext: Submitted application: Sample Spark Processing
[2025-01-16T14:08:11.424+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-01-16T14:08:11.431+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ResourceProfile: Limiting resource is cpu
[2025-01-16T14:08:11.431+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-01-16T14:08:11.465+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SecurityManager: Changing view acls to: ***
[2025-01-16T14:08:11.465+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SecurityManager: Changing modify acls to: ***
[2025-01-16T14:08:11.465+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SecurityManager: Changing view acls groups to:
[2025-01-16T14:08:11.465+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SecurityManager: Changing modify acls groups to:
[2025-01-16T14:08:11.465+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2025-01-16T14:08:11.671+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.
[2025-01-16T14:08:11.677+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.
[2025-01-16T14:08:11.680+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7079. Attempting port 7080.
[2025-01-16T14:08:11.683+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7080. Attempting port 7081.
[2025-01-16T14:08:11.687+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7081. Attempting port 7082.
[2025-01-16T14:08:11.694+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7082. Attempting port 7083.
[2025-01-16T14:08:11.698+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7083. Attempting port 7084.
[2025-01-16T14:08:11.701+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7084. Attempting port 7085.
[2025-01-16T14:08:11.715+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7085. Attempting port 7086.
[2025-01-16T14:08:11.718+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7086. Attempting port 7087.
[2025-01-16T14:08:11.731+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7087. Attempting port 7088.
[2025-01-16T14:08:11.733+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7088. Attempting port 7089.
[2025-01-16T14:08:11.737+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7089. Attempting port 7090.
[2025-01-16T14:08:11.739+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7090. Attempting port 7091.
[2025-01-16T14:08:11.742+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7091. Attempting port 7092.
[2025-01-16T14:08:11.745+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 WARN Utils: Service 'sparkDriver' could not bind on port 7092. Attempting port 7093.
[2025-01-16T14:08:11.751+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 ERROR SparkContext: Error initializing SparkContext.
[2025-01-16T14:08:11.752+0000] {spark_submit.py:491} INFO - java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (starting from 7077)! Consider explicitly setting the appropriate port for the service 'sparkDriver' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
[2025-01-16T14:08:11.752+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind0(Native Method)
[2025-01-16T14:08:11.752+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind(Net.java:459)
[2025-01-16T14:08:11.752+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind(Net.java:448)
[2025-01-16T14:08:11.752+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-01-16T14:08:11.753+0000] {spark_submit.py:491} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2025-01-16T14:08:11.775+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO SparkContext: Successfully stopped SparkContext
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,775 - ERROR - Error processing data: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - : java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (starting from 7077)! Consider explicitly setting the appropriate port for the service 'sparkDriver' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind0(Native Method)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind(Net.java:459)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind(Net.java:448)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
[2025-01-16T14:08:11.776+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2025-01-16T14:08:11.777+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:11.778+0000] {spark_submit.py:491} INFO - 2025-01-16 14:08:11,777 - ERROR - Traceback (most recent call last):
[2025-01-16T14:08:11.778+0000] {spark_submit.py:491} INFO - File "/opt/spark-apps/sample_spark_app.py", line 59, in main
[2025-01-16T14:08:11.778+0000] {spark_submit.py:491} INFO - spark = SparkSession.builder \
[2025-01-16T14:08:11.778+0000] {spark_submit.py:491} INFO - File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 269, in getOrCreate
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - sc = SparkContext.getOrCreate(sparkConf)
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 483, in getOrCreate
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - SparkContext(conf=conf or SparkConf())
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 197, in __init__
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - self._do_init(
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 282, in _do_init
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - self._jsc = jsc or self._initialize_context(self._conf._jconf)
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 402, in _initialize_context
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - return self._jvm.JavaSparkContext(jconf)
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1585, in __call__
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - return_value = get_return_value(
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - raise Py4JJavaError(
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - : java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (starting from 7077)! Consider explicitly setting the appropriate port for the service 'sparkDriver' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries.
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind0(Native Method)
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind(Net.java:459)
[2025-01-16T14:08:11.779+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.Net.bind(Net.java:448)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:11.780+0000] {spark_submit.py:491} INFO - 
[2025-01-16T14:08:11.810+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ShutdownHookManager: Shutdown hook called
[2025-01-16T14:08:11.811+0000] {spark_submit.py:491} INFO - 25/01/16 14:08:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-c61f1d78-1541-460c-8145-291862919fe5
[2025-01-16T14:08:11.926+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.driver.host=spark-master --conf spark.driver.bindAddress=spark-master --name arrow-spark --verbose /opt/spark-apps/sample_spark_app.py /opt/spark-data/input /opt/spark-data/output. Error code is: 1.
[2025-01-16T14:08:11.939+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=spark_processing_dag, task_id=spark_submit_job, execution_date=20250116T140201, start_date=20250116T140807, end_date=20250116T140811
[2025-01-16T14:08:11.960+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 68 for task spark_submit_job (Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.driver.host=spark-master --conf spark.driver.bindAddress=spark-master --name arrow-spark --verbose /opt/spark-apps/sample_spark_app.py /opt/spark-data/input /opt/spark-data/output. Error code is: 1.; 1240)
[2025-01-16T14:08:11.994+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-01-16T14:08:12.029+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
